{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"email_signature_168.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Quickdive - Machine Learning (ML)\n",
    "## München, 2020     \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Willkommen in **Ihrem persönlichen Jupyter-Notebook**.\n",
    "  \n",
    "Sie können in diesem Notebook alle Beispiele live nachvollziehen, aber auch eigene Varianten ausprobieren.  \n",
    "In der Menüleiste finden sich die wichtigsten Funktionen für \"Maus\"-User.  \n",
    "Hier noch einige sehr hilfreiche Tastatur-Kürzel für effizientes Arbeiten mit der Tastatur:\n",
    "\n",
    "* **Ausführen/Run** einer Zelle mit ... [SHIFT+ENTER]\n",
    "* Eine neue leere Zelle **über** einer Zelle einfügen mit ... [a] \n",
    "* Eine neue leere Zelle **unter** einer Zelle einfügen mit ... [b]\n",
    "* Eine Zelle **löschen/entfernen** !!VORSICHT!! mit ... [dd]\n",
    "* Eine Zelle in **Markdown-Format** umwandeln mit ... [m]\n",
    "* Eine Zelle in **Coding-Format** umwandeln mit ... [y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diesen Code müssen wir am Anfang IMMER ausführen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Grundausstattung an Bibliotheken, die wir immer laden\n",
    "import numpy as np                  # Numerische Operationen, Lineare Algebra\n",
    "from scipy.stats import *           # Funktionsbibliothek mit statistischen Funktionen\n",
    "import matplotlib.pyplot as plt     # Funktionsbilio<thek zur Visualisierung von Daten/Ergebnissen\n",
    "import pandas as pd                 # Bearbeitung von tabellarischen Daten (sog. Data Frames)\n",
    "import seaborn as sns               # Erweiterte Visualisierung von Daten/Ergebnissen etc.\n",
    "import warnings                     # Ermöglicht die Deaktivierung von best. Warnmeldungen\n",
    "import random                       # Damit kann man Zufallszahlen generieren\n",
    "import os                           # Ermöglicht Zugriff auf das Dateiablagesystem \n",
    "import datetime as dt               # Funktionsbiliothek zum Arbeiten mit Zeitreihen Daten\n",
    "import pickle                       # Ermöglicht das Abspeichern von Objekten (z.B. trainierten Modellen)\n",
    "\n",
    "# Ein paar Einstellungen, die einem das Leben einfacher machen\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['figure.figsize'] = [8, 4]\n",
    "plt.style.use('seaborn-white')\n",
    "from IPython.core.pylabtools import figsize\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "print(\"Los geht's ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Datenset: BMW-PRICING CHALLENGE\n",
    "\n",
    "Dafür bearbeiten wir jetzt ein praxisnäheres Beispiel: Das BMW-Pricing Challenge Datenset auf der Plattform KAGGLE  \n",
    "\n",
    "https://www.kaggle.com/danielkyrka/bmw-pricing-challenge \n",
    "\n",
    "Die Autoren dieses Datensets schreiben dazu:\n",
    "\n",
    "* With this challenge we hope to [...] gain some insight in what the main factors are that drive the value of a used car.  \n",
    "* The data provided consists of almost 5000 real BMW cars that were sold via a b2b auction in 2018.\n",
    "* The price shown in the table is the highest bid that was reached during the auction.\n",
    "* We have also extracted 8 criteria based on the equipment of car that we think might have a good impact on the value of a used car.\n",
    "* These criteria have been labeled feature1 to feature 8 and are shown in the data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zunächst laden wir die Rohdaten\n",
    "bmw = pd.read_csv(\"bmw_pricing_challenge.csv\")\n",
    "\n",
    "# Beschränkung auf die 20 am häufigsten vorkommenden Modelle\n",
    "t20_models = bmw.model_key.value_counts()[:20].index.to_list()  # Auslesen der T20 Modellbezeichnungen\n",
    "bmw = bmw.loc[bmw.model_key.isin(t20_models),:]  \n",
    "\n",
    "# Die beiden Datums-Merkmale 'sold_at' und 'registration_date' sollten wir besser in ein Datetime-Format konvertieren\n",
    "bmw.registration_date = pd.to_datetime(bmw.registration_date)\n",
    "bmw.sold_at = pd.to_datetime(bmw.sold_at)\n",
    "\n",
    "# Neue Datums-Features ableiten\n",
    "bmw[\"period\"] = bmw.sold_at - bmw.registration_date   # erstellt Spalte mit Differenz in Tagen\n",
    "bmw[\"period\"] = bmw.period.dt.days                    # normiert die Differenz in Tageseinheiten\n",
    "bmw[\"Sell_Month\"] = bmw.sold_at.dt.month              # Der Monat, in dem die Auktion stattfand\n",
    "\n",
    "bmw.reset_index(inplace=True)  \n",
    "bmw.shape\n",
    "bmw.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noch ein kurzer Blick auf die Verteilung der numerischen Variablen ...\n",
    "bmw.describe().round(1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schauen wir uns noch kurz die Verteilung des Fahrzeugalters an:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verteilung des Fahrzeugalters (in Jahren) im Datenset:\n",
    "_= (bmw.period/365).hist(bins=70, figsize=(8,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Betrachten wir die kategoriellen Features noch etwas genauer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(15,10)\n",
    "x,y = bmw.model_key, bmw.price\n",
    "_= sns.boxplot(x, y, data=bmw, color=\"tomato\") \n",
    "plt.title(\"Verteilung der Fahrzeugpreise nach Modellreihen\")\n",
    "plt.xticks(fontsize=14, rotation=80); plt.xlabel(\"Modellreihe\"), plt.ylabel(\"Preis\"); plt.ylim(0,70_000)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt bauen wir unsere Datenmatrix auf, auf der wir dann das Regressionsmodell trainieren wollen.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['model_key', 'mileage', 'engine_power','fuel', 'paint_color', 'car_type',\n",
    "            'feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5', 'feature_6', 'feature_7', 'feature_8',\n",
    "            'period', 'Sell_Month', ]\n",
    "\n",
    "\n",
    "X = bmw[features].copy()\n",
    "y = bmw.price.copy()\n",
    "\n",
    "##### (3) & (4) OH-Encoding und Standardisieren\n",
    "\n",
    "# Wir importieren die Preprocessing Tools aus Scikit-Learn\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler   # Unsere Werkzeuge\n",
    "\n",
    "# Wir legen ein paar Listen an, um das PreProcessing zu erleichtern\n",
    "feat_cat = [\"model_key\", \"fuel\", \"paint_color\", \"car_type\", ] \n",
    "feat_num = ['mileage', 'engine_power', 'period',]\n",
    "feat_bool = ['feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5', 'feature_6','feature_7', 'feature_8']\n",
    "feat_other = ['Sell_Month']\n",
    "\n",
    "# Jetzt vereinzeln wir die Matrix X in vier Teil-Matrizen \n",
    "Xcat   = X[feat_cat].copy()\n",
    "Xnum   = X[feat_num].copy()\n",
    "Xbool  = X[feat_bool].copy()\n",
    "Xother = X[feat_other].copy()\n",
    "\n",
    "# OH-Encoding auf der Matrix mit den kategoriellen Daten\n",
    "oh = OneHotEncoder(sparse=False)\n",
    "Xcat = oh.fit_transform(Xcat)\n",
    "Xcat_cols = oh.get_feature_names(feat_cat)\n",
    "Xcat = pd.DataFrame(data=Xcat, columns=Xcat_cols)\n",
    "\n",
    "# Standardisieren auf der Matrix mit den numerischen Daten\n",
    "scaler = StandardScaler()\n",
    "Xnum = scaler.fit_transform(Xnum)\n",
    "Xnum = pd.DataFrame(Xnum, columns=feat_num)\n",
    "\n",
    "# Zusammenführen der vier Teilmatrizen zu einer Datenmatrix X\n",
    "X = pd.concat([Xcat, Xnum, Xbool, Xother], axis=1,  )\n",
    "\n",
    "print(f\"Featurematrix X mit {X.shape[0]} Datensätzen und {X.shape[1]} Feature/Variablen\")\n",
    "print(f\"Targetvektor y mit {y.shape[0]} Datensätzen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainings- & Testset splitten: Wir splitten in ein Trainingsset mit 70% fürs Training und 30% fürs Testen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lineares Regressionsmodell trainieren "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression().fit(X_train, y_train)  # Model instanziieren und auf die Trainingsdaten trainieren\n",
    "\n",
    "scoreTrain = lr.score(X_train, y_train)        # Ermittelt R² Score für Trainingsdaten\n",
    "scoreTest  = lr.score(X_test, y_test)           # Ermittelt den R² für die Testdaten\n",
    "\n",
    "print(\"-\"*65)\n",
    "print(f\"Anteil der erklärbaren Varianz, R² auf dem Trainingsset  = {scoreTrain:.2f}\")\n",
    "print(f\"Anteil der erklärbaren Varianz, R² auf den TESTDATEN (!) = {scoreTest:.2f}\")\n",
    "print(\"-\"*65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd = SGDRegressor().fit(X_train, y_train)  # Model instanziieren und auf die Trainingsdaten trainieren\n",
    "\n",
    "scoreTrain = sgd.score(X_train, y_train)        # Ermittelt R² Score für Trainingsdaten\n",
    "scoreTest  = sgd.score(X_test, y_test)           # Ermittelt den R² für die Testdaten\n",
    "\n",
    "print(\"-\"*65)\n",
    "print(f\"Anteil der erklärbaren Varianz, R² auf dem Trainingsset  = {scoreTrain:.2f}\")\n",
    "print(f\"Anteil der erklärbaren Varianz, R² auf den TESTDATEN (!) = {scoreTest:.2f}\")\n",
    "print(\"-\"*65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe der einzelnen Faktoren mit ihren Gewichten in der Regression:\n",
    "weights = pd.Series(lr.coef_, index=X.columns.to_list(),)\n",
    "weights.sort_values(ascending=False)[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Mit unserem Modell können wir jetzt den Preis für \"neue\" ungesehene Daten schätzen:  \n",
    "Zur Vereinfachung ziehen wir uns aus unseren \"unberührten\" Testdaten ein Sample und lassen es durch unser Modell schätzen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Size = 5\n",
    "Sample = X_test.sample(Size, random_state=815)\n",
    "yreal = pd.Series(y_test[Sample.index])\n",
    "ypred = pd.Series(lr.predict(Sample), index=Sample.index, name=\"price_pred\").astype(\"int\")\n",
    "result = pd.concat([ypred,yreal,Sample], axis=1)\n",
    "result.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KAGGLE Competition - \"Give Me Some Credit\"\n",
    "https://www.kaggle.com/c/GiveMeSomeCredit/data\n",
    "\n",
    "Das schreiben die Autoren auf KAGGLE:\n",
    "\n",
    "*Credit scoring algorithms, which make a guess at the probability of default, are the method banks use to determine whether or not a loan should be granted.  \n",
    "This competition requires participants to improve on the state of the art in credit scoring, by predicting the probability that somebody will experience financial distress in the next two years.*\n",
    "\n",
    "*The goal of this competition is to build a model that borrowers can use to help make the best financial decisions.*\n",
    "\n",
    "Hier eine kurze Beschreibung der einzelnen Variablen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "cs_info = pd.read_excel(\"cs-Data Dictionary.xls\", header=1); cs_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datenset laden\n",
    "cs = pd.read_csv(\"cs-training-small.csv\")\n",
    "cs = cs.iloc[:,1:]\n",
    "cs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt werfen für mal einen Blick auf die Verteilung der Werte der einzelnen Variablen ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zumindest die RUULs liefern einen überdurchschnittlichen Erklärungsbeitrag für unser Modell.  \n",
    "Wir nehmen die auffälligen Merkmale mit in unsere weiteren Überlegungen.  \n",
    "Jetzt bauen wir unsere Datenmatrix X und unseren Targetvektor y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs.SeriousDlqin2yrs.value_counts()\n",
    "cs.SeriousDlqin2yrs.value_counts(normalize=True).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cs.iloc[:,:-1].copy()\n",
    "y = cs.iloc[:,-1]\n",
    "print(X.shape,y.shape)\n",
    "print(f\"Anteil Defaults im gesamten Datenset {y.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir splitten in ein Trainingsset (2/3) und ein Testset (1/3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33333, shuffle=True, stratify=y, random_state=123)\n",
    "print(f' Trainingsset: {X_train.shape, y_train.shape} / Test Set: {X_test.shape, y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification mit Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "\n",
    "model = 'Decision Tree'\n",
    "t_names = ['Kein Default', 'Default']\n",
    "\n",
    "estimator = DecisionTreeClassifier(class_weight=\"balanced\", ) # max_depth=5\n",
    "estimator.fit(X_train, y_train)\n",
    "\n",
    "ytrue = y_test\n",
    "ypred = estimator.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(ytrue, ypred)\n",
    "roc_auc = roc_auc_score(ytrue, ypred)\n",
    "print(f\"Dummy-Baseline Accuracy: {1-y_test.mean()}\")\n",
    "print(f'Accuracy Score: {accuracy:.4f}, AUC: {roc_auc:.4f}')\n",
    "print(\"\\n\",classification_report(ytrue, ypred, target_names=t_names))\n",
    "\n",
    "# Feature Importance aus Model in Dataframe FI schreiben\n",
    "fi_data = {'Feature': list(X_train.columns), 'F_Importance': estimator.feature_importances_}\n",
    "FI = pd.DataFrame(data=fi_data)\n",
    "FI = FI.sort_values('F_Importance', ascending=False); FI\n",
    "\n",
    "# Confusion Matrix erstellen\n",
    "mat = confusion_matrix(ytrue, ypred,)\n",
    "print(\"Confusion Matrix:\\n\",mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir können uns dazu auch eine Confusion-Matrix plotten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "category_names = [\"Kein Default\", \"Default\"]\n",
    "sns.heatmap(mat, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=category_names, yticklabels=category_names)\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.xlabel(\"Predicted\"); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit ein paar Optimierungen können wir bereits moderate/gute Ergebnisse erzielen.\n",
    "Nach diesen ersten \"Gehversuchen\" schicken wir ein paar weitere Modelle ins Rennen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification mit verschiedenen Modellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beim Decision Tree Classifier ist es nicht notwendig die Daten zu standardisieren.  \n",
    "Bei den Modellen, die wir jetzt zusätzlich ins Spiel bringen, könnte es sehr hilfreich sein.  \n",
    "Wir behalten uns diesen Preprocessing-Schritt noch vor und probieren es zunächst ohne Standardisierung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardisieren auf der Matrix mit den numerischen Daten\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir bauen uns ein Pipeline aus verschiedenen Classifiern, die wir in einem \"Durchgang\" auf unsere Trainings- und Testdaten anwenden werden.  \n",
    "Die einzelnen Schritte:\n",
    "\n",
    "+ Importieren der notwendigen Classifier Alogrithmen u. verschd. Werkzeuge.\n",
    "+ Instanziierung der einzelnen Algorithmen (so wird ein konkretes Learner-Objekt daraus).\n",
    "+ Erstellen einer Pipeline (Festlegen, welche Modelle tatsächlich angewendet werden sollen).\n",
    "+ Anlegen eines Dataframe, um die Ergebnisse der einzelnen Modelle abzuspeichern.\n",
    "+ Pipeline-Logik: Ruft die vorab defierten Classifier auf u. wendet sie auf X_train u. X_test an.\n",
    "+ Ausgeben der Ergebnisse aus unserem Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importieren der Classifier Algorithmen, die wir als Kandidaten verwenden möchten:\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Importieren von Metriken und Zeitfunktionen\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, classification_report\n",
    "import time\n",
    "\n",
    "# Hier sind unsere Classifier Kandidaten Modelle\n",
    "clf1 = GaussianNB()\n",
    "# clf2 = SVC(class_weight=\"balanced\",)\n",
    "clf2 = LinearSVC(class_weight=\"balanced\")\n",
    "clf3 = LogisticRegression(class_weight=\"balanced\")\n",
    "clf4 = KNeighborsClassifier()\n",
    "# Dem Random Forest spendieren wir 3 Varianten ...\n",
    "clf5 = RandomForestClassifier(class_weight=\"balanced\", n_jobs=-1)\n",
    "clf6 = RandomForestClassifier(n_estimators = 300, class_weight=\"balanced\", max_depth=3,  bootstrap=True, n_jobs=-1)\n",
    "clf7 = RandomForestClassifier(n_estimators = 500, class_weight=\"balanced\", max_depth=5,  bootstrap=False, n_jobs=-1)\n",
    "\n",
    "# Das ist unsere Pipeline die wir durchlaufen\n",
    "pipeline = [(1, \"NB\",clf1),\n",
    "           (2, \"LinSVM\", clf2),\n",
    "           (3, \"LogReg\", clf3),\n",
    "           (4, \"Knn5\", clf4),\n",
    "           (5, \"RF\", clf5),\n",
    "           (6, \"RF opt1\", clf6),\n",
    "           (7, \"RF opt2\", clf7),\n",
    "          ]  \n",
    "# Wir speichern die \"Rundenergebnisse\" der einzelnen Classifier in einem Dataframe\n",
    "results = pd.DataFrame( {\"Estimator\":[], \"Accuracy\":[], \"Precision\":[], \"Recall\":[], \"f1\":[], \"AUC\":[], \"Duration\":[]} )\n",
    "models_fitted = []  # Ablegen der gefitteten Modelle (Objekte) in einer Liste\n",
    "\n",
    "# Durchlauf mehrerer Modelle und Wegschreiben des Ergebnisses\n",
    "for i, name, estimator in pipeline:\n",
    "    \n",
    "    # Model fitten u. in Liste ablegen\n",
    "    print(f\"\\nFitting {name} ...\")\n",
    "    start = time.time()                     # Stoppuhr: Zwischenzeit nehmen\n",
    "    est = estimator.fit(X_train, y_train)   # model aus Listing nehmen und fitten\n",
    "    models_fitted.append(est)\n",
    "\n",
    "    # Scorings erstellen\n",
    "    ytrue = y_test                          # ...\n",
    "    ypred = est.predict(X_test)             # model auf Testdaten anwenden (predict)\n",
    "    \n",
    "    acc = accuracy_score(ytrue, ypred )     # Accuracy \n",
    "    prec = precision_score(ytrue, ypred )   # Precision \n",
    "    rec = recall_score(ytrue, ypred,  )     # Recall\n",
    "    f1 = f1_score(ytrue, ypred, )           # f1-Score\n",
    "    auc = roc_auc_score(ytrue, ypred, )     # AUC\n",
    "    end = time.time()                       # Stoppuhr: Zwischenzeit nehmen\n",
    "    duration = end - start                  # Walltime in Variable abspeichern\n",
    "    \n",
    "    results.loc[i,:] = [name, acc, prec, rec, f1, auc, duration]\n",
    "    print(f\"\\nFitting {name} took {duration:.1f} seconds\")\n",
    "    print(\"-\"*100)\n",
    "    print()\n",
    "    \n",
    "print(f\"Dummy-Baseline Accuracy: {1-y_test.mean()}\")\n",
    "results.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_not_normalized.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_not_normalized = results.copy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
